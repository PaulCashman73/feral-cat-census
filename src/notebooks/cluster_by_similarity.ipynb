{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhTF9y9vYOJWqQwcYwYVIu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60a15520a690410eae72038efb59668c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7c41cdc40bd4447b28030b9423bd830",
              "IPY_MODEL_0a036675bdab4c0ba9237afdc190cc2e",
              "IPY_MODEL_65fbe67310d4485fb413f11d80e28034"
            ],
            "layout": "IPY_MODEL_bdd6159905794dff85ea7c4eb7b78be4"
          }
        },
        "e7c41cdc40bd4447b28030b9423bd830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96a5fe55e37845b39a4bf1f0368975fb",
            "placeholder": "​",
            "style": "IPY_MODEL_460dc82f86ca440083f98fa0e26d0e8b",
            "value": "Batches: 100%"
          }
        },
        "0a036675bdab4c0ba9237afdc190cc2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17998c8455d54e8c957a0d9e9d5a2ab9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e9d09353f2e424bb9fe053f2cff572f",
            "value": 1
          }
        },
        "65fbe67310d4485fb413f11d80e28034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0484086031344bc99515a682bb61252a",
            "placeholder": "​",
            "style": "IPY_MODEL_c8f1e004491a47ae92bd3708ce45b434",
            "value": " 1/1 [00:23&lt;00:00, 23.42s/it]"
          }
        },
        "bdd6159905794dff85ea7c4eb7b78be4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a5fe55e37845b39a4bf1f0368975fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460dc82f86ca440083f98fa0e26d0e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17998c8455d54e8c957a0d9e9d5a2ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e9d09353f2e424bb9fe053f2cff572f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0484086031344bc99515a682bb61252a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8f1e004491a47ae92bd3708ce45b434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcashman21/feral-cat-census/blob/main/src/notebooks/cluster_by_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook performs the following steps:\n",
        "\n",
        "\n",
        "1.   Run YOLO v8 on all raw images to get cropped-image files based on detecting either entire cats or just cat faces/heads.\n",
        "2.   If necessary, run a step to check that all the cropped-image files are usable.  In the case of cat faces, we assume they all are.  (Actually, we can filter out those that aren't based on file name.) In the case of full cat bodies, run each image through a model that can determine usability.\n",
        "3.   Run a transformer to compute a dense vector representation of all the cropped images, and compute a similarity score for each pair of images.\n",
        "4.   Assign cluster IDs first to all the images that are above a given similarity score threshold, and then to all remaining unclustered images (each of which becomes a singleton cluster).  The number of clusters is the estimated number of distinct cats.\n",
        "\n"
      ],
      "metadata": {
        "id": "VqWfOeScgh5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB8eOHe8gbph",
        "outputId": "0a5158f7-2471-4f37-ef89-d408449119fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.0.199)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (17.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (17.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=278b8787e7e4906963119c08510912c48b80731a7ba756807fe6c47cddb237ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "# for loading/processing the images\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# models\n",
        "from keras.models import Model, load_model\n",
        "from keras import layers\n",
        "\n",
        "# for everything else\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "# YOLO\n",
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# transformer\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rks5VB0SkSj",
        "outputId": "72764bc9-af04-4753-f3d0-897fa2381c53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to trained YOLO model for image detection (cats or cat faces)\n",
        "path_to_best_cat_detector = '/content/gdrive/My Drive/Cat images/feral-cat-faces.v1i.yolov8/runs/detect/train/weights/best.pt'\n",
        "cat_detector_model = YOLO(path_to_best_cat_detector)  # load the cat (or cat face) detector model\n",
        "\n",
        "# Path to trained model for detecting usable images.  If set to '', then we are simply accepting all images\n",
        "path_to_usable_image_selector_model = '' # '/content/gdrive/My Drive/Cat images/models/select_usable_images.keras'\n",
        "if path_to_usable_image_selector_model == '':\n",
        "  usable_image_selector_model = None\n",
        "else:\n",
        "  usable_image_selector_model = load_model(path_to_usable_image_selector_model) # load usable/unsuable image selector model\n",
        "\n",
        "# Name of object class for YOLO to detect ('cat' for entire cat in any pose, 'cat-face' for just cat faces/heads)\n",
        "class_to_detect = 'cat-face' # 'cat' for whole cat, 'cat-face' for just faces"
      ],
      "metadata": {
        "id": "8JRu8gi8eXfZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the zip file of cat images into the working storage of the notebook by clicking the folder in the left nav, and then the upload symbol in the menu bar.  Adjust the `path_to_images` variable to point to the unzipped folder."
      ],
      "metadata": {
        "id": "xyU7BMXsh6QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to raw images of cats (BE SURE TO UPLOAD AND UNZIP)\n",
        "path_to_images = '/content/nano'\n",
        "zip_file_name = path_to_images +'.zip'\n",
        "\n",
        "\n",
        "import zipfile\n",
        "!unzip $zip_file_name\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS2kcKXPh1iU",
        "outputId": "e7e3bf1e-ec04-4abd-9308-cbca7656bdc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/nano.zip\n",
            "   creating: nano/\n",
            "  inflating: __MACOSX/._nano         \n",
            "  inflating: nano/275-Marcel-508-222-Marcel-432-161002_DSC_0388.jpg.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._275-Marcel-508-222-Marcel-432-161002_DSC_0388.jpg.jpg.jpg  \n",
            "  inflating: nano/758-Sahara-IMG_4792.JPG.jpg  \n",
            "  inflating: __MACOSX/nano/._758-Sahara-IMG_4792.JPG.jpg  \n",
            "  inflating: nano/236-Flyers-446-160928_DSC_0029.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._236-Flyers-446-160928_DSC_0029.jpg.jpg  \n",
            "  inflating: nano/758-Sahara-IMG_4954.JPG.jpg  \n",
            "  inflating: __MACOSX/nano/._758-Sahara-IMG_4954.JPG.jpg  \n",
            "  inflating: nano/258-Farley-494-170201_DSC_2321-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._258-Farley-494-170201_DSC_2321-2.jpg.jpg  \n",
            "  inflating: nano/232-Zetta_-photo-output_152.JPG.jpg  \n",
            "  inflating: __MACOSX/nano/._232-Zetta_-photo-output_152.JPG.jpg  \n",
            "  inflating: nano/238-Flyers-448-160928_DSC_0048.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._238-Flyers-448-160928_DSC_0048.jpg.jpg  \n",
            "  inflating: nano/252-Osborn-468-161009_DSC_0644.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._252-Osborn-468-161009_DSC_0644.jpg.jpg  \n",
            "  inflating: nano/248-Darcy_-479-161012_DSC_0759.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._248-Darcy_-479-161012_DSC_0759.jpg.jpg  \n",
            "  inflating: nano/244-Mason-170611_DSC_1679-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._244-Mason-170611_DSC_1679-2.jpg.jpg  \n",
            "  inflating: nano/.DS_Store          \n",
            "  inflating: __MACOSX/nano/._.DS_Store  \n",
            "  inflating: nano/206-Mack-416-160929_DSC_0284-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._206-Mack-416-160929_DSC_0284-2.jpg.jpg  \n",
            "  inflating: nano/217-Raina-427-161002_DSC_0382.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._217-Raina-427-161002_DSC_0382.jpg.jpg  \n",
            "  inflating: nano/249-Darcy -465-160929_DSC_0247.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._249-Darcy -465-160929_DSC_0247.jpg.jpg  \n",
            "  inflating: nano/234-Zetta_-photo-output_151.JPG.jpg  \n",
            "  inflating: __MACOSX/nano/._234-Zetta_-photo-output_151.JPG.jpg  \n",
            "  inflating: nano/279-Marlow-512-170201_DSC_2274.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._279-Marlow-512-170201_DSC_2274.jpg.jpg  \n",
            "  inflating: nano/277-Addiso-510-202-Addiso-412-160929_DSC_0269.jpg.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._277-Addiso-510-202-Addiso-412-160929_DSC_0269.jpg.jpg.jpg  \n",
            "  inflating: nano/260-Rose-488-161018_12027499_1016014948428937_1340200475861970431_n.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._260-Rose-488-161018_12027499_1016014948428937_1340200475861970431_n.jpg.jpg  \n",
            "  inflating: nano/253-Jolene-469-161009_DSC_0642.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._253-Jolene-469-161009_DSC_0642.jpg.jpg  \n",
            "  inflating: nano/242-Mercer-452-161004_DSC_0540.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._242-Mercer-452-161004_DSC_0540.jpg.jpg  \n",
            "  inflating: nano/771-Sahara-200626_DSC_3718.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._771-Sahara-200626_DSC_3718.jpg.jpg  \n",
            "  inflating: nano/205-Chanel-415-160930_DSC_0291.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._205-Chanel-415-160930_DSC_0291.jpg.jpg  \n",
            "  inflating: nano/263-Chanel-15440475_1287461311328730_3703842762570776666_o.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._263-Chanel-15440475_1287461311328730_3703842762570776666_o.jpg.jpg  \n",
            "  inflating: nano/232-Zetta_-IMG_1026.jpeg.jpg  \n",
            "  inflating: __MACOSX/nano/._232-Zetta_-IMG_1026.jpeg.jpg  \n",
            "  inflating: nano/209-Nano W-461-160928_DSC_0107.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._209-Nano W-461-160928_DSC_0107.jpg.jpg  \n",
            "  inflating: nano/255-Raffer-483-161011_DSC_0676-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._255-Raffer-483-161011_DSC_0676-2.jpg.jpg  \n",
            "  inflating: nano/219-Olive-429-161002_DSC_0414.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._219-Olive-429-161002_DSC_0414.jpg.jpg  \n",
            "  inflating: nano/248-Darcy_-170112_DSC_1512-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._248-Darcy_-170112_DSC_1512-2.jpg.jpg  \n",
            "  inflating: nano/232-Zetta_-442-161002_DSC_0469.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._232-Zetta_-442-161002_DSC_0469.jpg.jpg  \n",
            "  inflating: nano/196-Max-406-160928_DSC_0137-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._196-Max-406-160928_DSC_0137-2.jpg.jpg  \n",
            "  inflating: nano/273-Emory-506-243-Emory-454-161004_DSC_0521.jpg.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._273-Emory-506-243-Emory-454-161004_DSC_0521.jpg.jpg.jpg  \n",
            "  inflating: nano/209-Nano-419-161001_DSC_0306-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._209-Nano-419-161001_DSC_0306-2.jpg.jpg  \n",
            "  inflating: nano/249-Darcy_-170112_DSC_1512.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._249-Darcy_-170112_DSC_1512.jpg.jpg  \n",
            "  inflating: nano/210-Radley-420-161002_DSC_0402.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._210-Radley-420-161002_DSC_0402.jpg.jpg  \n",
            "  inflating: nano/262-Unknow-487-161018_DSC_0847-3.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._262-Unknow-487-161018_DSC_0847-3.jpg.jpg  \n",
            "  inflating: nano/271-Merrit-504-170201_DSC_2209-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._271-Merrit-504-170201_DSC_2209-2.jpg.jpg  \n",
            "  inflating: nano/282-Morrow-515-170131_DSC_1941.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._282-Morrow-515-170131_DSC_1941.jpg.jpg  \n",
            "  inflating: nano/233-Zetta_-443-161002_DSC_0474.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._233-Zetta_-443-161002_DSC_0474.jpg.jpg  \n",
            "  inflating: nano/257-Chanel-491-161023_DSC_1010.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._257-Chanel-491-161023_DSC_1010.jpg.jpg  \n",
            "  inflating: nano/213-Floyd-423-161002_DSC_0365.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._213-Floyd-423-161002_DSC_0365.jpg.jpg  \n",
            "  inflating: nano/212-Fiona-422-161002_DSC_0372.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._212-Fiona-422-161002_DSC_0372.jpg.jpg  \n",
            "  inflating: nano/263-Chanel-492-161022_DSC_1000-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._263-Chanel-492-161022_DSC_1000-2.jpg.jpg  \n",
            "  inflating: nano/234-Zetta_-444-161002_DSC_0479.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._234-Zetta_-444-161002_DSC_0479.jpg.jpg  \n",
            "  inflating: nano/226-Nat-457-161002_DSC_0430.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._226-Nat-457-161002_DSC_0430.jpg.jpg  \n",
            "  inflating: nano/255-Raffer-484-161011_DSC_0701.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._255-Raffer-484-161011_DSC_0701.jpg.jpg  \n",
            "  inflating: nano/244-Mason-161001_PICT0786.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._244-Mason-161001_PICT0786.jpg.jpg  \n",
            "  inflating: nano/304-Esmere-DSC_4916.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._304-Esmere-DSC_4916.jpg.jpg  \n",
            "  inflating: nano/234-Zetta_-IMG_1027.jpeg.jpg  \n",
            "  inflating: __MACOSX/nano/._234-Zetta_-IMG_1027.jpeg.jpg  \n",
            "  inflating: nano/208-Zetta-418-160928_DSC_0101.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._208-Zetta-418-160928_DSC_0101.jpg.jpg  \n",
            "  inflating: nano/317-Mikey-161011_DSC_0675.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._317-Mikey-161011_DSC_0675.jpg.jpg  \n",
            "  inflating: nano/259-Maris-493-170131_DSC_2051.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._259-Maris-493-170131_DSC_2051.jpg.jpg  \n",
            "  inflating: nano/218-Dana-428-161002_DSC_0394.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._218-Dana-428-161002_DSC_0394.jpg.jpg  \n",
            "  inflating: nano/232-Zetta_-IMG_1023.jpeg.jpg  \n",
            "  inflating: __MACOSX/nano/._232-Zetta_-IMG_1023.jpeg.jpg  \n",
            "  inflating: nano/270-Mali-503-221-Mali-431-161002_DSC_0381.jpg.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._270-Mali-503-221-Mali-431-161002_DSC_0381.jpg.jpg.jpg  \n",
            "  inflating: nano/767-Sahara-200626_DSC_3737.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._767-Sahara-200626_DSC_3737.jpg.jpg  \n",
            "  inflating: nano/382-Kestre-12792317_1018119834929547_2579333533442259492_o.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._382-Kestre-12792317_1018119834929547_2579333533442259492_o.jpg.jpg  \n",
            "  inflating: nano/250-Morgan-480-161011_DSC_0706.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._250-Morgan-480-161011_DSC_0706.jpg.jpg  \n",
            "  inflating: nano/249-Darcy_-478-161012_DSC_0753.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._249-Darcy_-478-161012_DSC_0753.jpg.jpg  \n",
            "  inflating: nano/244-Mason-170805_DSC_3200.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._244-Mason-170805_DSC_3200.jpg.jpg  \n",
            "  inflating: nano/230-Beatri-440-161002_DSC_0442.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._230-Beatri-440-161002_DSC_0442.jpg.jpg  \n",
            "  inflating: nano/269-Nala-502-lg_220-Nala-430-161002_DSC_0359.jpg.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._269-Nala-502-lg_220-Nala-430-161002_DSC_0359.jpg.jpg.jpg  \n",
            "  inflating: nano/246-Flynn-459-161004_DSC_0496.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._246-Flynn-459-161004_DSC_0496.jpg.jpg  \n",
            "  inflating: nano/199-Enzo-409-160929_DSC_0237.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._199-Enzo-409-160929_DSC_0237.jpg.jpg  \n",
            "  inflating: nano/768-Sahara-200626_DSC_3776.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._768-Sahara-200626_DSC_3776.jpg.jpg  \n",
            "  inflating: nano/276-Marley-509-224-Marley-434-161002_DSC_0404.jpg.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._276-Marley-509-224-Marley-434-161002_DSC_0404.jpg.jpg.jpg  \n",
            "  inflating: nano/245-Malta-458-161004_DSC_0517.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._245-Malta-458-161004_DSC_0517.jpg.jpg  \n",
            "  inflating: nano/216-Orion-426-161002_DSC_0366.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._216-Orion-426-161002_DSC_0366.jpg.jpg  \n",
            "  inflating: nano/235-Zetta_-445-161002_DSC_0466.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._235-Zetta_-445-161002_DSC_0466.jpg.jpg  \n",
            "  inflating: nano/305-Marcel-543-FullSizeRender.jpg-1.jpeg.jpg  \n",
            "  inflating: __MACOSX/nano/._305-Marcel-543-FullSizeRender.jpg-1.jpeg.jpg  \n",
            "  inflating: nano/239-Marlin-449-160928_DSC_0097.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._239-Marlin-449-160928_DSC_0097.jpg.jpg  \n",
            "  inflating: nano/257-Chanel-170112_DSC_1424.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._257-Chanel-170112_DSC_1424.jpg.jpg  \n",
            "  inflating: nano/769-Sahara-200626_DSC_3748.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._769-Sahara-200626_DSC_3748.jpg.jpg  \n",
            "  inflating: nano/200-Ember-410-160929_DSC_0239.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._200-Ember-410-160929_DSC_0239.jpg.jpg  \n",
            "  inflating: nano/198-Deacon-408-160929_DSC_0233-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._198-Deacon-408-160929_DSC_0233-2.jpg.jpg  \n",
            "  inflating: nano/268-Racer-501-170131_DSC_1882-3.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._268-Racer-501-170131_DSC_1882-3.jpg.jpg  \n",
            "  inflating: nano/770-Sahara-200626_DSC_3723.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._770-Sahara-200626_DSC_3723.jpg.jpg  \n",
            "  inflating: nano/304-Esmere-542-FullSizeRender.jpg-2.jpeg.jpg  \n",
            "  inflating: __MACOSX/nano/._304-Esmere-542-FullSizeRender.jpg-2.jpeg.jpg  \n",
            "  inflating: nano/261-Finste-486-161018_DSC_0847-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._261-Finste-486-161018_DSC_0847-2.jpg.jpg  \n",
            "  inflating: nano/237-Flyers-447-160928_DSC_0043.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._237-Flyers-447-160928_DSC_0043.jpg.jpg  \n",
            "  inflating: nano/244-Mason-161002_PICT0116.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._244-Mason-161002_PICT0116.jpg.jpg  \n",
            "  inflating: nano/272-Owen-505-170201_DSC_2199-3.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._272-Owen-505-170201_DSC_2199-3.jpg.jpg  \n",
            "  inflating: nano/215-Fabio-425-160929_DSC_0266.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._215-Fabio-425-160929_DSC_0266.jpg.jpg  \n",
            "  inflating: nano/204-Darcy-414-160929_DSC_0261.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._204-Darcy-414-160929_DSC_0261.jpg.jpg  \n",
            "  inflating: nano/244-Mason-455-161004_DSC_0510.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._244-Mason-455-161004_DSC_0510.jpg.jpg  \n",
            "  inflating: nano/283-Mitt-516-170201_DSC_2208.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._283-Mitt-516-170201_DSC_2208.jpg.jpg  \n",
            "  inflating: nano/280-Aviva-513-170201_DSC_2388.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._280-Aviva-513-170201_DSC_2388.jpg.jpg  \n",
            "  inflating: nano/201-Ada-411-160929_DSC_0269-3.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._201-Ada-411-160929_DSC_0269-3.jpg.jpg  \n",
            "  inflating: nano/207-Arya-417-161002_DSC_0456.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._207-Arya-417-161002_DSC_0456.jpg.jpg  \n",
            "  inflating: nano/203-Adele-413-160929_DSC_0269-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._203-Adele-413-160929_DSC_0269-2.jpg.jpg  \n",
            "  inflating: nano/274-Maddox-507-211-Maddox-421-161002_DSC_0433.jpg.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._274-Maddox-507-211-Maddox-421-161002_DSC_0433.jpg.jpg.jpg  \n",
            "  inflating: nano/240-Fitz-450-160929_DSC_0197.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._240-Fitz-450-160929_DSC_0197.jpg.jpg  \n",
            "  inflating: nano/248-Darcy -464-160929_DSC_0249-2.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._248-Darcy -464-160929_DSC_0249-2.jpg.jpg  \n",
            "  inflating: nano/232-Zetta_-IMG_1025.jpeg.jpg  \n",
            "  inflating: __MACOSX/nano/._232-Zetta_-IMG_1025.jpeg.jpg  \n",
            "  inflating: nano/216-Orion-470-161009_DSC_0658.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._216-Orion-470-161009_DSC_0658.jpg.jpg  \n",
            "  inflating: nano/226-Nat-456-161004_DSC_0565.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._226-Nat-456-161004_DSC_0565.jpg.jpg  \n",
            "  inflating: nano/244-Mason-160929_PICT0124.jpg.jpg  \n",
            "  inflating: __MACOSX/nano/._244-Mason-160929_PICT0124.jpg.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_cropped_images = '/content/runs/detect/cropped_images_2'\n",
        "path_to_cropped_and_classified_images = os.path.join(path_to_cropped_images, class_to_detect)"
      ],
      "metadata": {
        "id": "0HeOCCUHkfGN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_case_images = []\n",
        "df = pd.DataFrame(columns=['orig_image_filename', 'cropped_image_filename', 'cluster_id'])\n",
        "\n",
        "# Read all image files\n",
        "with os.scandir(path_to_images) as files:\n",
        "  for file in files:\n",
        "    last_component = file.name.split('.')[-1]\n",
        "    if last_component == 'jpg' or last_component == 'jpeg':\n",
        "      test_case_images.append(file.name)\n",
        "\n",
        "# Run the YOLOv8 model (the cat detector) and get a YOLO Results object for\n",
        "# each file named in the df\n",
        "# https://docs.ultralytics.com/modes/predict/#working-with-results\n",
        "root_filename = 'img'\n",
        "root_filename_count = 0\n",
        "for test_case in test_case_images:\n",
        "  results = cat_detector_model.predict(source=os.path.join(path_to_images, test_case))\n",
        "  # Extract the image sub areas and save them as files\n",
        "  for r in results:\n",
        "    cropped_image_file_name = root_filename + str(root_filename_count) + '.jpg'\n",
        "    df = pd.concat([df, pd.DataFrame([{'orig_image_filename': test_case, 'cropped_image_filename': cropped_image_file_name, 'cluster_id': -1 }], index=[len(df)])],  axis=0)\n",
        "    r.save_crop(save_dir=path_to_cropped_images, file_name=cropped_image_file_name)\n",
        "    root_filename_count += 1\n",
        "\n",
        "if usable_image_selector_model is not None:\n",
        "\n",
        "  # The cat detector isn't perfect.  It finds cats where there are none, and\n",
        "  # doesn't always split up multiple cats into individual images.  So run the\n",
        "  # YOLOv8 resulting cropped image files through the usable/unusable model\n",
        "  # and delete any files that don't have a single cat.\n",
        "\n",
        "  # We're reading raw images from the cropped image files, so transform them\n",
        "  # by normalizing the pixels and resizing the image\n",
        "  transformer_nn = tf.keras.Sequential([\n",
        "        layers.Resizing(244,244),\n",
        "        layers.Rescaling(1./255)])\n",
        "\n",
        "  # Compile a list of all the cropped images\n",
        "  cropped_image_files = []\n",
        "  unusable_images_count = 0\n",
        "  with os.scandir(path_to_cropped_and_classified_images) as files:\n",
        "    for file in files:\n",
        "      # YOLO will save cropped images which are unusable.  These have YOLO-generated names\n",
        "      # that don't appear in the df.  If we are given such a file to process, ignore it.\n",
        "      if file.name not in df['cropped_image_filename'].values:\n",
        "        continue\n",
        "\n",
        "      image = imread(os.path.join(path_to_cropped_and_classified_images, file.name)) # Get the cropped image\n",
        "      # Transform the image by resizing and rescaling\n",
        "      image_resized = transformer_nn(image)\n",
        "      # Convert tensor to numpy array\n",
        "      image_resized = image_resized.numpy()\n",
        "      # Turn the image into a batch of 1\n",
        "      image_batch = np.expand_dims(image_resized, axis=0)\n",
        "      # Predict the batch outcome\n",
        "      is_usable = usable_image_selector_model.predict(image_batch)[0] > .5\n",
        "      if is_usable:\n",
        "        cropped_image_files.append(file.name)\n",
        "      else:\n",
        "        print('Unusable image: {}'.format(file.name))\n",
        "        unusable_images_count += 1\n",
        "        df = df.drop(df.index[df['cropped_image_filename'] == file.name], axis=0)\n",
        "  print('Total of {} unusable images and {} usable images'.format(unusable_images_count, len(cropped_image_files)))\n",
        "else:\n",
        "  print('Total of {} usable images'.format(len(df)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdYIFUKHz71q",
        "outputId": "bf0fa361-7bb1-43de-af2a-a233cb5afdec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/nano/232-Zetta_-IMG_1025.jpeg.jpg: 640x576 1 cat-face, 283.7ms\n",
            "Speed: 15.6ms preprocess, 283.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/nano/246-Flynn-459-161004_DSC_0496.jpg.jpg: 640x640 1 cat-face, 380.0ms\n",
            "Speed: 11.5ms preprocess, 380.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/382-Kestre-12792317_1018119834929547_2579333533442259492_o.jpg.jpg: 640x640 1 cat-face, 384.2ms\n",
            "Speed: 7.4ms preprocess, 384.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/226-Nat-456-161004_DSC_0565.jpg.jpg: 640x640 1 cat-face, 352.1ms\n",
            "Speed: 7.1ms preprocess, 352.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/213-Floyd-423-161002_DSC_0365.jpg.jpg: 640x640 1 cat-face, 406.9ms\n",
            "Speed: 6.8ms preprocess, 406.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/245-Malta-458-161004_DSC_0517.jpg.jpg: 640x640 1 cat-face, 328.3ms\n",
            "Speed: 5.9ms preprocess, 328.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/275-Marcel-508-222-Marcel-432-161002_DSC_0388.jpg.jpg.jpg: 640x640 2 cat-faces, 318.5ms\n",
            "Speed: 5.8ms preprocess, 318.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/199-Enzo-409-160929_DSC_0237.jpg.jpg: 640x640 1 cat-face, 361.8ms\n",
            "Speed: 7.2ms preprocess, 361.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/216-Orion-426-161002_DSC_0366.jpg.jpg: 640x640 1 cat-face, 394.6ms\n",
            "Speed: 9.7ms preprocess, 394.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/276-Marley-509-224-Marley-434-161002_DSC_0404.jpg.jpg.jpg: 640x640 1 cat-face, 260.3ms\n",
            "Speed: 5.1ms preprocess, 260.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/218-Dana-428-161002_DSC_0394.jpg.jpg: 640x640 1 cat-face, 235.1ms\n",
            "Speed: 5.2ms preprocess, 235.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/257-Chanel-491-161023_DSC_1010.jpg.jpg: 640x640 1 cat-face, 234.7ms\n",
            "Speed: 5.0ms preprocess, 234.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/305-Marcel-543-FullSizeRender.jpg-1.jpeg.jpg: 640x640 1 cat-face, 227.6ms\n",
            "Speed: 7.9ms preprocess, 227.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/279-Marlow-512-170201_DSC_2274.jpg.jpg: 640x640 1 cat-face, 245.3ms\n",
            "Speed: 4.5ms preprocess, 245.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/201-Ada-411-160929_DSC_0269-3.jpg.jpg: 640x640 1 cat-face, 261.2ms\n",
            "Speed: 4.5ms preprocess, 261.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/270-Mali-503-221-Mali-431-161002_DSC_0381.jpg.jpg.jpg: 640x640 1 cat-face, 249.3ms\n",
            "Speed: 6.7ms preprocess, 249.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/235-Zetta_-445-161002_DSC_0466.jpg.jpg: 640x640 1 cat-face, 228.7ms\n",
            "Speed: 5.1ms preprocess, 228.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/217-Raina-427-161002_DSC_0382.jpg.jpg: 640x640 1 cat-face, 235.1ms\n",
            "Speed: 5.4ms preprocess, 235.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/261-Finste-486-161018_DSC_0847-2.jpg.jpg: 640x640 1 cat-face, 262.0ms\n",
            "Speed: 5.7ms preprocess, 262.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/255-Raffer-484-161011_DSC_0701.jpg.jpg: 640x640 1 cat-face, 245.1ms\n",
            "Speed: 6.8ms preprocess, 245.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/203-Adele-413-160929_DSC_0269-2.jpg.jpg: 640x640 1 cat-face, 239.4ms\n",
            "Speed: 4.7ms preprocess, 239.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/248-Darcy -464-160929_DSC_0249-2.jpg.jpg: 640x640 2 cat-faces, 218.7ms\n",
            "Speed: 4.6ms preprocess, 218.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/205-Chanel-415-160930_DSC_0291.jpg.jpg: 640x640 1 cat-face, 273.8ms\n",
            "Speed: 6.1ms preprocess, 273.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/274-Maddox-507-211-Maddox-421-161002_DSC_0433.jpg.jpg.jpg: 640x640 2 cat-faces, 241.5ms\n",
            "Speed: 5.1ms preprocess, 241.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/238-Flyers-448-160928_DSC_0048.jpg.jpg: 640x640 1 cat-face, 232.5ms\n",
            "Speed: 5.3ms preprocess, 232.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/283-Mitt-516-170201_DSC_2208.jpg.jpg: 640x640 1 cat-face, 265.6ms\n",
            "Speed: 6.1ms preprocess, 265.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/206-Mack-416-160929_DSC_0284-2.jpg.jpg: 640x640 1 cat-face, 245.3ms\n",
            "Speed: 5.4ms preprocess, 245.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/257-Chanel-170112_DSC_1424.jpg.jpg: 640x640 1 cat-face, 356.8ms\n",
            "Speed: 7.0ms preprocess, 356.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/234-Zetta_-444-161002_DSC_0479.jpg.jpg: 640x640 1 cat-face, 370.4ms\n",
            "Speed: 6.8ms preprocess, 370.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/252-Osborn-468-161009_DSC_0644.jpg.jpg: 640x640 1 cat-face, 356.9ms\n",
            "Speed: 7.1ms preprocess, 356.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/269-Nala-502-lg_220-Nala-430-161002_DSC_0359.jpg.jpg.jpg: 640x640 1 cat-face, 433.8ms\n",
            "Speed: 6.9ms preprocess, 433.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/258-Farley-494-170201_DSC_2321-2.jpg.jpg: 640x640 1 cat-face, 371.7ms\n",
            "Speed: 5.2ms preprocess, 371.7ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/272-Owen-505-170201_DSC_2199-3.jpg.jpg: 640x640 1 cat-face, 355.6ms\n",
            "Speed: 5.0ms preprocess, 355.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/240-Fitz-450-160929_DSC_0197.jpg.jpg: 640x640 1 cat-face, 360.0ms\n",
            "Speed: 8.0ms preprocess, 360.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/250-Morgan-480-161011_DSC_0706.jpg.jpg: 640x640 1 cat-face, 215.3ms\n",
            "Speed: 4.2ms preprocess, 215.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/260-Rose-488-161018_12027499_1016014948428937_1340200475861970431_n.jpg.jpg: 640x640 1 cat-face, 242.8ms\n",
            "Speed: 5.2ms preprocess, 242.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/768-Sahara-200626_DSC_3776.jpg.jpg: 640x640 1 cat-face, 268.7ms\n",
            "Speed: 5.9ms preprocess, 268.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/232-Zetta_-442-161002_DSC_0469.jpg.jpg: 640x640 1 cat-face, 240.0ms\n",
            "Speed: 5.1ms preprocess, 240.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/273-Emory-506-243-Emory-454-161004_DSC_0521.jpg.jpg.jpg: 640x640 1 cat-face, 230.7ms\n",
            "Speed: 5.2ms preprocess, 230.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/244-Mason-161001_PICT0786.jpg.jpg: 480x640 1 cat-face, 181.3ms\n",
            "Speed: 4.4ms preprocess, 181.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/nano/237-Flyers-447-160928_DSC_0043.jpg.jpg: 640x640 1 cat-face, 232.1ms\n",
            "Speed: 5.2ms preprocess, 232.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/249-Darcy_-170112_DSC_1512.jpg.jpg: 416x640 4 cat-faces, 146.8ms\n",
            "Speed: 3.9ms preprocess, 146.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/nano/208-Zetta-418-160928_DSC_0101.jpg.jpg: 640x640 2 cat-faces, 211.4ms\n",
            "Speed: 4.3ms preprocess, 211.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/244-Mason-161002_PICT0116.jpg.jpg: 480x640 1 cat-face, 156.5ms\n",
            "Speed: 3.4ms preprocess, 156.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/nano/234-Zetta_-IMG_1027.jpeg.jpg: 640x544 2 cat-faces, 194.1ms\n",
            "Speed: 3.3ms preprocess, 194.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/nano/769-Sahara-200626_DSC_3748.jpg.jpg: 640x640 3 cat-faces, 219.9ms\n",
            "Speed: 4.4ms preprocess, 219.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/234-Zetta_-photo-output_151.JPG.jpg: 640x640 1 cat-face, 231.8ms\n",
            "Speed: 5.4ms preprocess, 231.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/280-Aviva-513-170201_DSC_2388.jpg.jpg: 640x640 1 cat-face, 209.0ms\n",
            "Speed: 5.4ms preprocess, 209.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/271-Merrit-504-170201_DSC_2209-2.jpg.jpg: 640x640 1 cat-face, 264.4ms\n",
            "Speed: 4.7ms preprocess, 264.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/317-Mikey-161011_DSC_0675.jpg.jpg: 640x640 1 cat-face, 237.9ms\n",
            "Speed: 8.5ms preprocess, 237.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/770-Sahara-200626_DSC_3723.jpg.jpg: 640x640 2 cat-faces, 254.6ms\n",
            "Speed: 5.6ms preprocess, 254.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/268-Racer-501-170131_DSC_1882-3.jpg.jpg: 640x640 1 cat-face, 244.1ms\n",
            "Speed: 4.4ms preprocess, 244.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/248-Darcy_-170112_DSC_1512-2.jpg.jpg: 640x640 1 cat-face, 418.9ms\n",
            "Speed: 6.9ms preprocess, 418.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/226-Nat-457-161002_DSC_0430.jpg.jpg: 640x640 1 cat-face, 380.7ms\n",
            "Speed: 8.5ms preprocess, 380.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/232-Zetta_-IMG_1023.jpeg.jpg: 640x640 1 cat-face, 387.5ms\n",
            "Speed: 5.1ms preprocess, 387.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/758-Sahara-IMG_4792.JPG.jpg: 640x640 1 cat-face, 349.6ms\n",
            "Speed: 6.6ms preprocess, 349.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/282-Morrow-515-170131_DSC_1941.jpg.jpg: 640x640 1 cat-face, 249.7ms\n",
            "Speed: 4.3ms preprocess, 249.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/232-Zetta_-photo-output_152.JPG.jpg: 640x640 1 cat-face, 220.8ms\n",
            "Speed: 4.6ms preprocess, 220.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/200-Ember-410-160929_DSC_0239.jpg.jpg: 640x640 1 cat-face, 296.8ms\n",
            "Speed: 5.9ms preprocess, 296.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/212-Fiona-422-161002_DSC_0372.jpg.jpg: 640x640 1 cat-face, 225.4ms\n",
            "Speed: 5.1ms preprocess, 225.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/209-Nano-419-161001_DSC_0306-2.jpg.jpg: 640x640 1 cat-face, 222.5ms\n",
            "Speed: 5.5ms preprocess, 222.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/204-Darcy-414-160929_DSC_0261.jpg.jpg: 640x640 1 cat-face, 250.7ms\n",
            "Speed: 5.2ms preprocess, 250.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/253-Jolene-469-161009_DSC_0642.jpg.jpg: 640x640 1 cat-face, 269.5ms\n",
            "Speed: 12.0ms preprocess, 269.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/771-Sahara-200626_DSC_3718.jpg.jpg: 640x640 1 cat-face, 281.1ms\n",
            "Speed: 8.0ms preprocess, 281.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/219-Olive-429-161002_DSC_0414.jpg.jpg: 640x640 1 cat-face, 257.6ms\n",
            "Speed: 5.3ms preprocess, 257.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/242-Mercer-452-161004_DSC_0540.jpg.jpg: 640x640 1 cat-face, 242.1ms\n",
            "Speed: 7.3ms preprocess, 242.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/198-Deacon-408-160929_DSC_0233-2.jpg.jpg: 640x640 1 cat-face, 228.5ms\n",
            "Speed: 4.8ms preprocess, 228.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/239-Marlin-449-160928_DSC_0097.jpg.jpg: 640x640 2 cat-faces, 237.5ms\n",
            "Speed: 6.4ms preprocess, 237.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/249-Darcy -465-160929_DSC_0247.jpg.jpg: 640x640 1 cat-face, 236.2ms\n",
            "Speed: 4.4ms preprocess, 236.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/196-Max-406-160928_DSC_0137-2.jpg.jpg: 640x640 1 cat-face, 230.9ms\n",
            "Speed: 5.0ms preprocess, 230.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/232-Zetta_-IMG_1026.jpeg.jpg: 640x544 2 cat-faces, 1145.2ms\n",
            "Speed: 17.4ms preprocess, 1145.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/nano/215-Fabio-425-160929_DSC_0266.jpg.jpg: 640x640 1 cat-face, 371.4ms\n",
            "Speed: 7.2ms preprocess, 371.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/244-Mason-160929_PICT0124.jpg.jpg: 480x640 (no detections), 318.1ms\n",
            "Speed: 5.7ms preprocess, 318.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img72.jpg was written\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/nano/263-Chanel-15440475_1287461311328730_3703842762570776666_o.jpg.jpg: 640x640 1 cat-face, 399.7ms\n",
            "Speed: 8.5ms preprocess, 399.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/277-Addiso-510-202-Addiso-412-160929_DSC_0269.jpg.jpg.jpg: 640x640 1 cat-face, 350.3ms\n",
            "Speed: 5.3ms preprocess, 350.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/207-Arya-417-161002_DSC_0456.jpg.jpg: 640x640 1 cat-face, 349.4ms\n",
            "Speed: 6.5ms preprocess, 349.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/230-Beatri-440-161002_DSC_0442.jpg.jpg: 640x640 1 cat-face, 368.0ms\n",
            "Speed: 6.8ms preprocess, 368.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/304-Esmere-542-FullSizeRender.jpg-2.jpeg.jpg: 640x640 (no detections), 233.3ms\n",
            "Speed: 5.2ms preprocess, 233.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/244-Mason-170805_DSC_3200.jpg.jpg: 640x640 1 cat-face, 217.3ms\n",
            "Speed: 4.5ms preprocess, 217.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/236-Flyers-446-160928_DSC_0029.jpg.jpg: 640x640 1 cat-face, 219.9ms\n",
            "Speed: 5.1ms preprocess, 219.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/259-Maris-493-170131_DSC_2051.jpg.jpg: 640x640 1 cat-face, 238.5ms\n",
            "Speed: 5.5ms preprocess, 238.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/758-Sahara-IMG_4954.JPG.jpg: 640x640 1 cat-face, 244.0ms\n",
            "Speed: 5.3ms preprocess, 244.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/262-Unknow-487-161018_DSC_0847-3.jpg.jpg: 640x640 1 cat-face, 236.7ms\n",
            "Speed: 4.1ms preprocess, 236.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/209-Nano W-461-160928_DSC_0107.jpg.jpg: 640x640 1 cat-face, 256.0ms\n",
            "Speed: 4.3ms preprocess, 256.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/304-Esmere-DSC_4916.jpg.jpg: 640x640 1 cat-face, 225.7ms\n",
            "Speed: 5.1ms preprocess, 225.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/244-Mason-170611_DSC_1679-2.jpg.jpg: 640x640 2 cat-faces, 217.5ms\n",
            "Speed: 7.0ms preprocess, 217.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/210-Radley-420-161002_DSC_0402.jpg.jpg: 640x640 1 cat-face, 231.9ms\n",
            "Speed: 5.0ms preprocess, 231.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/233-Zetta_-443-161002_DSC_0474.jpg.jpg: 640x640 1 cat-face, 238.2ms\n",
            "Speed: 6.3ms preprocess, 238.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/767-Sahara-200626_DSC_3737.jpg.jpg: 640x640 1 cat-face, 243.9ms\n",
            "Speed: 5.5ms preprocess, 243.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/255-Raffer-483-161011_DSC_0676-2.jpg.jpg: 640x640 1 cat-face, 211.6ms\n",
            "Speed: 4.2ms preprocess, 211.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/249-Darcy_-478-161012_DSC_0753.jpg.jpg: 640x640 2 cat-faces, 224.2ms\n",
            "Speed: 4.1ms preprocess, 224.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/248-Darcy_-479-161012_DSC_0759.jpg.jpg: 640x640 1 cat-face, 209.6ms\n",
            "Speed: 4.3ms preprocess, 209.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/244-Mason-455-161004_DSC_0510.jpg.jpg: 640x640 1 cat-face, 252.5ms\n",
            "Speed: 5.1ms preprocess, 252.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/263-Chanel-492-161022_DSC_1000-2.jpg.jpg: 640x640 1 cat-face, 227.5ms\n",
            "Speed: 5.0ms preprocess, 227.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/nano/216-Orion-470-161009_DSC_0658.jpg.jpg: 640x640 1 cat-face, 348.8ms\n",
            "Speed: 5.8ms preprocess, 348.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of 95 usable images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Why can files be missing?\n",
        "for file in df['cropped_image_filename'].values:\n",
        "    if not os.path.isfile(os.path.join(path_to_cropped_and_classified_images, file)):\n",
        "      print('Missing file: {}'.format(file))\n",
        "      df = df.drop(df.index[df['cropped_image_filename'] == file], axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU7WoBSNx9_9",
        "outputId": "558ebd45-b2f6-4213-89e3-8900ed2e5c0f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing file: img72.jpg\n",
            "Missing file: img77.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owlWAW7Oz2ia",
        "outputId": "669ef764-d4de-4c69-fba6-dd231ecbd9b3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the OpenAI CLIP Model\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "\n",
        "# Next we compute the embeddings\n",
        "image_names = df['cropped_image_filename'].values # All the names of YOLO-produced cropped images\n",
        "encoded_image = model.encode([Image.open(os.path.join(path_to_cropped_and_classified_images, filepath)) for filepath in image_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# Now we run the clustering algorithm. This function compares images against\n",
        "# all other images and returns a list with the pairs that have the highest\n",
        "# cosine similarity score\n",
        "processed_images = util.paraphrase_mining_embeddings(encoded_image)\n",
        "df_processed_images = pd.DataFrame(processed_images, columns=['score', 'image_id1', 'image_id2'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "60a15520a690410eae72038efb59668c",
            "e7c41cdc40bd4447b28030b9423bd830",
            "0a036675bdab4c0ba9237afdc190cc2e",
            "65fbe67310d4485fb413f11d80e28034",
            "bdd6159905794dff85ea7c4eb7b78be4",
            "96a5fe55e37845b39a4bf1f0368975fb",
            "460dc82f86ca440083f98fa0e26d0e8b",
            "17998c8455d54e8c957a0d9e9d5a2ab9",
            "6e9d09353f2e424bb9fe053f2cff572f",
            "0484086031344bc99515a682bb61252a",
            "c8f1e004491a47ae92bd3708ce45b434"
          ]
        },
        "id": "J6o_0Au2odEH",
        "outputId": "e1a0f6a9-5ec5-4ee4-b18c-6599109636b6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60a15520a690410eae72038efb59668c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_THRESHOLD = 0.95\n",
        "next_cluster_id = 1\n",
        "df['cluster_id'] = -1\n",
        "\n",
        "\n",
        "def does_pair_form_cluster(row):\n",
        "    if row['score'] >= CLUSTER_THRESHOLD:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def get_cluster_image_names(row):\n",
        "    return [image_names[row['image_id1']], image_names[row['image_id2']]]\n",
        "\n",
        "\n",
        "def assign_cluster_id(row):\n",
        "  global next_cluster_id\n",
        "  if row['is_cluster']:\n",
        "      file1, file2 = get_cluster_image_names(row)\n",
        "      idx1 = df[df['cropped_image_filename'] == file1].index\n",
        "      idx2 = df[df['cropped_image_filename'] == file2].index\n",
        "      if len(idx1) == 1 and len(idx2) == 1:\n",
        "        if int(df['cluster_id'].loc[idx1]) > -1:\n",
        "          df['cluster_id'].loc[idx2] = int(df['cluster_id'].loc[idx1])\n",
        "        elif int(df['cluster_id'].loc[idx2]) > -1:\n",
        "          df['cluster_id'].loc[idx1] = int(df['cluster_id'].loc[idx2])\n",
        "        else:\n",
        "          df['cluster_id'].loc[idx1] = int(next_cluster_id)\n",
        "          df['cluster_id'].loc[idx2] = int(next_cluster_id)\n",
        "          next_cluster_id += 1\n",
        "\n",
        "def assign_unclustered_images(row):\n",
        "  global next_cluster_id\n",
        "  if row['cluster_id'] == -1:\n",
        "      # row['cluster_id'] = next_cluster_id\n",
        "      next_cluster_id += 1\n",
        "      return next_cluster_id - 1\n",
        "  else:\n",
        "      return row['cluster_id']"
      ],
      "metadata": {
        "id": "nvJ6hCnNq6mz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine which image pairs are close enough to belong to the same cluster.  If a row in\n",
        "# df_processed_images can be clustered, mark it as true.\n",
        "df_processed_images['is_cluster'] = df_processed_images.apply(does_pair_form_cluster, axis=1)\n",
        "\n",
        "# For all the clusterable pairs, assign cluster IDs in df (NOT df_processed_images)\n",
        "df_processed_images.apply(assign_cluster_id, axis=1)\n",
        "\n",
        "# Finally, assign cluster IDs to all images which have not been put into a cluster already.\n",
        "# These are singleton images.\n",
        "df['cluster_id'] = df.apply(assign_unclustered_images, axis=1)\n",
        "\n",
        "# How many distinct clusters (cats) are there?\n",
        "df['cluster_id'].max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qALyYCCJrhIJ",
        "outputId": "4ac53180-c91b-41ce-e29b-ab50fe3a3f7f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}